# This is your custom AI assistant configuration for Continue
# It is configured to use a local Ollama model

name: My Local Assistant
version: 1.0.0
schema: v1

# Models define which AI models this assistant can use
models:
  - title: "Local DeepSeek Coder"  # A friendly name shown in the UI
    provider: "ollama"             # Tells Continue to use the Ollama provider
    model: "deepseek-coder"        # Must match your pulled Ollama model name
    # apiBase is optional for local Ollama, as it defaults to localhost:11434
    # apiBase: "http://localhost:11434" 

# Context providers define what information the assistant can access
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase
